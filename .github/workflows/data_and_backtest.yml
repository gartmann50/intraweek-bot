name: Polygon Data (Incremental) + Backtest

on:
  workflow_dispatch:
    inputs:
      update_data:
        description: "Update Polygon data before backtest?"
        type: boolean
        default: false
  schedule:
    - cron: "20 2 * * 1-5"   # weekdays 02:20 UTC

jobs:
  data:
    if: ${{ inputs.update_data }}         # run only when toggled on
    name: Update Polygon data (incremental)
    runs-on: ubuntu-latest
    env:
      DATA_DIR: stock_data_500

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install --upgrade pip
          pip install requests pandas

      - name: Compute 4y window
        run: |
          echo "START_DATE=$(date -u -d '4 years ago' +%F)" >> $GITHUB_ENV
          echo "END_DATE=$(date -u +%F)" >> $GITHUB_ENV

      - name: Restore data cache
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.DATA_DIR }}
          key: polygon-data-v1
          restore-keys: |
            polygon-data-v1

      - name: Show pre-update contents
        run: |
          mkdir -p "${DATA_DIR}"
          echo "Files before update:"
          ls -1 "${DATA_DIR}" | head -n 20 || true

      - name: Build 500 largest by turnover (incremental)
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          python tools/make_universe_polygon.py \
            --out-dir "${DATA_DIR}" \
            --start "${START_DATE}" \
            --end "${END_DATE}" \
            --universe-size 500

      - name: Save data cache
        uses: actions/cache/save@v4
        with:
          path: ${{ env.DATA_DIR }}
          key: polygon-data-v1

      - name: Upload data artifact (for this run)
        uses: actions/upload-artifact@v4
        with:
          name: stock_data_500
          path: ${{ env.DATA_DIR }}/**

  backtest:
    name: Backtest
    runs-on: ubuntu-latest
    # run regardless; it will either use the artifact from 'data' or the cached folder
    needs: [data]
    if: ${{ always() }}   # still run when data job is skipped (toggle off)
    env:
      DATA_DIR: stock_data_500
      OUT_DIR: backtests

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install --upgrade pip
          pip install pandas numpy

      # If data job ran, we have an artifact â€“ download it
      - name: Download data artifact (when updated)
        if: ${{ needs.data.result == 'success' }}
        uses: actions/download-artifact@v4
        with:
          name: stock_data_500
          path: ${{ env.DATA_DIR }}

      # If no data job (toggle off), restore the cached dataset
      - name: Restore data cache (when not updated)
        if: ${{ needs.data.result != 'success' }}
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.DATA_DIR }}
          key: polygon-data-v1
          restore-keys: |
            polygon-data-v1

      - name: Verify data exists
        run: |
          test -d "${DATA_DIR}" && ls -1 "${DATA_DIR}" | head -n 5 || \
          (echo "No data found. Run with 'update_data: true' at least once to seed the cache." && exit 1)

      - name: Compute 4y window (explicit dates)
        run: |
          echo "START_DATE=$(date -u -d '4 years ago' +%F)" >> $GITHUB_ENV
          echo "END_DATE=$(date -u +%F)" >> $GITHUB_ENV

      - name: Run RSI weekly backtest (no inference surprises)
        run: |
          python tools/backtest_weekly_rsi.py \
            --data-dir "${DATA_DIR}" \
            --start "${START_DATE}" \
            --end   "${END_DATE}" \
            --out-dir "${OUT_DIR}"

      - name: Upload backtest artifact
        uses: actions/upload-artifact@v4
        with:
          name: backtests
          path: ${{ env.OUT_DIR }}/**
