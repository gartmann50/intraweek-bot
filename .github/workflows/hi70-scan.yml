name: IW Bot â€” 70D Highs Scan

on:
  schedule:
    - cron: "15 21 * * 5"   # ~after close Fri (EDT)
    - cron: "15 20 * * 5"   # ~after close Fri (EST)
  workflow_dispatch:
    inputs:
      anchor_friday:
        description: "Override anchor Friday (YYYY-MM-DD)"
        required: false
        default: ""
      use_current_week:
        description: "Scan current Mon..today (true/false)"
        required: false
        default: "false"
      top:
        description: "How many names to keep in CSV"
        required: false
        default: "10"
      topvol:
        description: "Keep top-N by dollar volume before scanning"
        required: false
        default: "1000"
      cap_min:
        description: "Min market cap USD (0=disable)"
        required: false
        default: "1000000000"
      min_price:
        description: "Min last close"
        required: false
        default: "5"
      min_dollar_vol:
        description: "Min dollar volume (close*volume)"
        required: false
        default: "10000000"

permissions:
  contents: read

concurrency:
  group: iwbot-hi70-scan
  cancel-in-progress: true

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests pyyaml

      - name: Compute last Friday (NY)
        id: when
        shell: python
        run: |
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo
          ny = ZoneInfo("America/New_York")
          d = datetime.now(ny).date()
          while d.weekday()!=4:
              d -= timedelta(days=1)
          open("${{ github.output }}","w").write(f"friday={d}\n")

      # Writes the updated scanner into the runner and compile-checks it.
      - name: Hotfix: write scan_70d_highs.py (supports --topvol) and compile
        shell: bash
        run: |
          set -e
          mkdir -p tools
          cat > tools/scan_70d_highs.py <<'PY'
#!/usr/bin/env python3
from __future__ import annotations
import argparse, csv, os, sys, time, requests
from dataclasses import dataclass
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs
from collections import Counter
try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo  # type: ignore

NY = ZoneInfo("America/New_York")
POLY_TICKERS = "https://api.polygon.io/v3/reference/tickers"
POLY_AGGS    = "https://api.polygon.io/v2/aggs/ticker/{sym}/range/1/day/{start}/{end}"
POLY_GROUPED = "https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/{d}"

def ny_today() -> date: return datetime.now(NY).date()
def last_friday_ny() -> date:
    d = ny_today()
    while d.weekday()!=4: d -= timedelta(days=1)
    return d
def ymd(d: date) -> str: return d.isoformat()
def backoff_sleep(k: int) -> None: time.sleep(min(0.5*(2**k),6.0))
def http_get_json(url: str, params: Dict[str,str], max_tries: int = 6) -> Dict:
    last=None
    for k in range(max_tries):
        try:
            r=requests.get(url, params=params, timeout=30)
            if r.status_code==429: backoff_sleep(k); continue
            r.raise_for_status(); return r.json() or {}
        except requests.HTTPError as e:
            last=e
            if r.status_code>=500: backoff_sleep(k); continue
            break
        except Exception as e:
            last=e; backoff_sleep(k)
    raise RuntimeError(f"GET failed {url} ({last})")

def list_common_stocks_all(api_key: str, on_date: Optional[date]) -> List[Dict]:
    params={"market":"stocks","active":"true","type":"CS","limit":"1000","apiKey":api_key}
    if on_date: params["date"]=ymd(on_date)
    results: List[Dict]=[]; cursor=None
    while True:
        p=dict(params)
        if cursor: p["cursor"]=cursor
        j=http_get_json(POLY_TICKERS, p)
        results.extend(j.get("results",[]) or [])
        nxt=j.get("next_url") or ""
        cursor=parse_qs(urlparse(nxt).query).get("cursor",[None])[0] if nxt else None
        if not cursor: break
    return results

def fetch_grouped_daily(api_key: str, d: date) -> Dict[str, Tuple[float,float,float]]:
    url=POLY_GROUPED.format(d=ymd(d)); params={"adjusted":"true","apiKey":api_key}
    j=http_get_json(url, params); data={}
    for r in j.get("results",[]) or []:
        sym=str(r.get("T") or "").upper()
        c=float(r.get("c",0) or 0); v=float(r.get("v",0) or 0)
        if sym: data[sym]=(c,v,c*v)
    return data

def fetch_daily_bars(api_key: str, symbol: str, start: date, end: date) -> List[Dict]:
    url=POLY_AGG_
