name: Weekly Picks

on:
  schedule:

    # After U.S. close (Mon–Thu) — update trailing stops
    - cron: "15 20 * * 1-4"   # EDT
    - cron: "15 21 * * 1-4"   # EST

    # Friday after close — build next week's picks / email
    - cron: "15 20 * * 5"     # EDT
    - cron: "15 21 * * 5"     # EST

    # Weekday opens (Mon–Fri) — sells daily; buys only first session after Fri
    - cron: "28 13 * * 1-5"   # EDT
    - cron: "28 14 * * 1-5"   # EST

  workflow_dispatch:
    inputs:
      mode:
        description: "status | build | email_smoke | build_and_email"
        required: true
        default: build_and_email
        type: choice
        options: [status, build, email_smoke, build_and_email]
      until:
        description: "Override Friday (YYYY-MM-DD). Empty = auto (Europe/Oslo)"
        required: false
        type: string
      topk:
        description: "How many names to show/email"
        required: false
        default: "6"
        type: string
      run_open:
        description: "Run MARKET-OPEN path now (manual test)"
        required: true
        type: boolean
        default: false
      force_buy:
        description: "Force weekly buy even if not first session (manual test)"
        required: true
        type: boolean
        default: false

      
concurrency:
  group: weekly-picks
  cancel-in-progress: true

jobs:
  main:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    # Defaults for scheduled runs
    env:
      MODE:  ${{ github.event_name == 'schedule' && 'build_and_email' || inputs.mode }}
      TOPK:  ${{ inputs.topk != '' && inputs.topk || '6' }}
      UNTIL: ${{ inputs.until }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml requests

      - name: Echo trigger
        shell: bash
        run: |
          echo "Triggered by: ${{ github.event_name }} at $(/bin/date -u '+%Y-%m-%d %H:%M:%S') UTC"

      - name: Compute last Friday (Europe/Oslo) + write runner_dates.json
        id: when
        run: |
          python - <<'PY'
          from datetime import datetime, timedelta
          import zoneinfo, json, os
          tz = zoneinfo.ZoneInfo("Europe/Oslo")
          now = datetime.now(tz)
          override = "${{ inputs.until }}".strip()
          if override:
            fri = override
          else:
            d = now.date()
            while d.weekday() != 4:  # 0=Mon,...,4=Fri
              d -= timedelta(days=1)
            fri = d.isoformat()
          fdate = datetime.fromisoformat(fri).date()
          mon = (fdate + timedelta(days=3)).isoformat()  # nominal Monday (may be holiday)
          with open('runner_dates.json','w') as f:
            json.dump({"friday": fri, "nominal_monday": mon, "generated_at": now.isoformat()}, f, indent=2)
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
            f.write(f"friday={fri}\n")
            f.write(f"monday={mon}\n")
          print("FRIDAY=", fri)
          print("MONDAY=", mon)
          PY

      - name: Cache dataset between runs
        uses: actions/cache@v4
        with:
          path: stock_data_400
          key: polygon-dataset-${{ steps.when.outputs.friday }}-max500
          restore-keys: |
            polygon-dataset-

      - name: Fetch dataset for curated universe (build stock_data_400.zip)
        env:
          POLY_KEY: ${{ secrets.POLYGON_API_KEY }}
          FRIDAY:   ${{ steps.when.outputs.friday }}
          SINCE:    "2023-01-01"
        shell: python
        run: |
          import os, csv, time, zipfile, pathlib, datetime as dt, requests, pandas as pd
          key=os.getenv("POLY_KEY"); friday=os.getenv("FRIDAY"); since=os.getenv("SINCE","2023-01-01")
          if not key: raise SystemExit("Missing POLYGON_API_KEY")

          # 1) read curated allow-list (must exist)
          u=pathlib.Path("universe/top400_weekly.csv")
          if not u.exists(): raise SystemExit("universe/top400_weekly.csv not found (commit it to the repo)")
          syms=pd.read_csv(u)["symbol"].astype(str).str.upper().dropna().unique().tolist()
          print(f"Curated universe: {len(syms)} symbols from {u}")

          # 2) download/append daily bars for each symbol
          ddir=pathlib.Path("stock_data_400"); ddir.mkdir(exist_ok=True)
          headers=["Date","Open","High","Low","Close","Volume"]
          agg="https://api.polygon.io/v2/aggs/ticker/{sym}/range/1/day/{start}/{end}"

          def fetch(sym, start, end):
            for k in range(6):
              r=requests.get(agg.format(sym=sym,start=start,end=end),
                             params={"adjusted":"true","limit":50000,"sort":"asc","apiKey":key}, timeout=30)
              if r.status_code!=429:
                if r.status_code==404: return []  # skip unknown/delisted
                r.raise_for_status(); return r.json().get("results",[]) or []
              time.sleep(0.5*(2**k))
            return []

          added=0; updated=0
          for i,s in enumerate(syms,1):
            p=ddir/f"{s}.csv"
            start=since
            if p.exists():
              try:
                last=pd.read_csv(p,usecols=[0]).iloc[-1,0]
                start=(pd.to_datetime(last).date()+dt.timedelta(days=1)).isoformat()
              except Exception: start=since
            if start>friday: continue
            rows=fetch(s, start, friday)
            if not rows: continue
            mode="a" if p.exists() and start!=since else "w"
            with p.open(mode, newline="") as f:
              w=csv.writer(f)
              if mode=="w": w.writerow(headers)
              for bar in rows:
                d=dt.datetime.utcfromtimestamp(bar["t"]/1000).date().isoformat()
                w.writerow([d,bar["o"],bar["h"],bar["l"],bar["c"],bar["v"]])
                added+=1
            updated+=1
            if i%50==0: print(f"{i}/{len(syms)} processed …")
            time.sleep(0.05)

          # 3) zip for the next step
          z=pathlib.Path("stock_data_400.zip")
          with zipfile.ZipFile(z,"w",compression=zipfile.ZIP_DEFLATED) as zipf:
            for p in ddir.glob("*.csv"): zipf.write(p, arcname=p.name)
          print(f"Built {z} | files={len(list(ddir.glob('*.csv')))} | updated_syms={updated} | new_rows={added}")

      - name: Unpack EOD dataset if zip present
        run: |
          if [ -f stock_data_400.zip ]; then
            rm -rf stock_data_400
            mkdir -p stock_data_400
            unzip -q -o stock_data_400.zip -d stock_data_400
            # move any nested CSVs up to the root folder
            find stock_data_400 -type f -name '*.csv' ! -path 'stock_data_400/*' -exec mv -f {} stock_data_400/ \; || true
            # clean empty dirs
            find stock_data_400 -type d -mindepth 1 -empty -delete || true
            echo "Unzipped CSV count:" $(ls stock_data_400/*.csv 2>/dev/null | wc -l)
          else
            echo "No stock_data_400.zip found (nothing to unpack)."
          fi
     
      - name: Generate equities-only universe (from dataset + exclude list)
        shell: python
        run: |
          import os, csv, glob, pathlib, datetime as dt
          import pandas as pd

          data_dir = "stock_data_400"
          uni_dir  = pathlib.Path("universe"); uni_dir.mkdir(exist_ok=True, parents=True)
          exclude_file = uni_dir / "exclude.txt"
          out_file = uni_dir / "top400_weekly.generated.csv"

          # Make sure exclude list exists (you can edit it in the repo anytime)
          if not exclude_file.exists():
              exclude_file.write_text(
                  "# One symbol per line (UPPERCASE). ETFs/funds/delisted here are dropped every run.\n"
                  "SPY\nQQQ\nIWM\nVOO\nVTI\nDIA\nXLK\nXLE\nXLV\nXLF\n"
                  "SPLK\nSGEN\n", encoding="utf-8"
              )

          # 1) collect symbols from filenames in dataset (handles TICKER.csv or TICKER_*.csv)
          symbols = set()
          for p in glob.glob(os.path.join(data_dir, "*.csv")):
              base = pathlib.Path(p).stem
              sym = base.split("_")[0].upper()
              symbols.add(sym)

          # 2) remove excluded
          exclude = {line.strip().upper() for line in exclude_file.read_text(encoding="utf-8").splitlines()
                     if line.strip() and not line.strip().startswith("#")}
          symbols -= exclude

          # 3) drop stale/delisted: if last row date is older than cutoff (e.g., 120 days before dataset max)
          last_dates = {}
          dataset_last = dt.date(1970,1,1)
          for sym in list(symbols):
              # try both TICKER.csv and TICKER_*.csv
              candidates = sorted(glob.glob(os.path.join(data_dir, f"{sym}*.csv")))
              if not candidates: continue
              try:
                  df = pd.read_csv(candidates[0])
                  if df.empty: continue
                  # assume first column is date
                  d = pd.to_datetime(df.iloc[-1, 0], errors="coerce")
                  if pd.notna(d):
                      dd = d.date()
                      last_dates[sym] = dd
                      if dd > dataset_last: dataset_last = dd
              except Exception:
                  pass

          cutoff = dataset_last - dt.timedelta(days=120)
          symbols = {s for s in symbols if s in last_dates and last_dates[s] >= cutoff}

          # 4) write universe CSV
          with out_file.open("w", newline="") as f:
              w = csv.DictWriter(f, fieldnames=["symbol"])
              w.writeheader()
              for s in sorted(symbols):
                  w.writerow({"symbol": s})

          print(f"Universe → {out_file}  kept={len(symbols)}  excluded={len(exclude)}  cutoff={cutoff}")
          if len(symbols) == 0:
              raise SystemExit("Universe is empty after filtering; aborting.")

      - name: Sanity — count & peek one CSV
        shell: python
        run: |
          import glob, pandas as pd
          files = sorted(glob.glob('stock_data_400/*.csv'))
          print("Found", len(files), "CSV files in stock_data_400/")
          if not files:
             raise SystemExit("No CSVs found after unzip")

          print("Sample:", files[0])
          df = pd.read_csv(files[0]).head()
          print(df.to_string(index=False))

      # --- BUILD PICKLIST ---
      - name: Build weekly picklist (RSI≥68, rank=rsi, ext20≤0.15, Top-40/wk)
        run: |
          python build_picklist_parametric.py \
            --data-dir stock_data_400 \
            --rsi-min 68 \
            --max-ext20 0.15 \
            --top-per-week 40 \
            --until "${UNTIL}" \
            --out backtests/picklist_highrsi_trend.csv

      - name: Filter to US Common Stock (type=CS) + cap floor, then re-rank (compact)
        env:
          POLY_KEY: ${{ secrets.POLYGON_API_KEY }}
          FRIDAY:   ${{ steps.when.outputs.friday }}
          CAP_MIN:  "10000000000"   # $10B; bump to 20000000000 for $20B if you like
          EXCLUDE_SYMS: "ATLCL,SPLK,SGEN,SPY,QQQ,IWM,VOO,VTI,DIA,XLK"
        shell: python
        run: |
          import os, time, requests, pandas as pd, sys
          pick='backtests/picklist_highrsi_trend.csv'
          try: df=pd.read_csv(pick)
          except Exception as e: sys.exit(f'Could not read {pick}: {e}')
          if df.empty: sys.exit('Picklist is empty.')
          wk='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not wk: sys.exit('No week column.')

          key=os.getenv('POLY_KEY'); friday=os.getenv('FRIDAY'); cap_min=float(os.getenv('CAP_MIN','0'))
          if not key: sys.exit('No POLYGON_API_KEY secret')

          # ban list (defaults + repo file + env)
          ban={s.strip().upper() for s in os.getenv('EXCLUDE_SYMS','').split(',') if s.strip()}
          try:
            with open('universe/exclude.txt',encoding='utf-8') as f:
              ban |= {l.strip().upper() for l in f if l.strip() and not l.strip().startswith('#')}
          except FileNotFoundError:
            pass

          # latest week subset
          df['symbol']=df['symbol'].astype(str).str.upper()
          d=pd.to_datetime(df[wk], errors='coerce').dt.date
          latest=d.max()
          cand=df[d==latest].copy()
          uniq=list(cand['symbol'].dropna().unique())
          # query Polygon overview only for candidates (fast)
          def overview(sym):
            u=f'https://api.polygon.io/v3/reference/tickers/{sym}'
            for k in range(5):
              r=requests.get(u, params={'date':friday,'apiKey':key}, timeout=20)
              if r.status_code==404: return ('NA', 0.0)      # treat as not allowed
              if r.status_code!=429:
                r.raise_for_status()
                j=(r.json() or {}).get('results',{}) or {}
                return (j.get('type'), float(j.get('market_cap') or 0.0))
              time.sleep(0.5*(2**k))
            return ('ERR', 0.0)

          keep=set(); dropped=[]
          for s in uniq:
            if s in ban: dropped.append((s,'BANNED',None)); continue
            t, mc = overview(s)
            if t=='CS' and mc>=cap_min: keep.add(s)
            else: dropped.append((s,t,mc))

          before=len(cand)
          cand=cand[cand['symbol'].isin(keep)].copy()
          removed=before-len(cand)

          # enforce your RSI ordering and renumber ranks for the latest week
          if 'rank' in cand.columns:
            cand['rank']=pd.to_numeric(cand['rank'], errors='coerce')
            cand=cand.sort_values(['rank','symbol'], ascending=[True,True], kind='stable')
          elif 'score' in cand.columns:
            cand['score']=pd.to_numeric(cand['score'], errors='coerce')
            cand=cand.sort_values(['score','symbol'], ascending=[False,True], kind='stable')
          cand['rank']=range(1, len(cand)+1)

          # stitch back into the full file
          out=pd.concat([df[d!=latest], cand], ignore_index=True)
          out.to_csv(pick, index=False)

          print(f'Kept {len(keep)} symbols with type=CS & market_cap >= {int(cap_min):,}. Removed {removed}.')
          if dropped:
            print('Dropped (sample):', dropped[:10])
          print('Top-6 after filter:', ','.join(cand.head(6)['symbol'].astype(str)))

      - name: Debug — show latest-week candidates (sorted) + save CSV
        shell: python
        run: |
          import sys, pandas as pd, numpy as np
          p='backtests/picklist_highrsi_trend.csv'
          try:
              df=pd.read_csv(p)
          except Exception as e:
              sys.exit(f'Could not read {p}: {e}')
          if df.empty: sys.exit('Picklist is empty.')

          # detect week column
          wk='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not wk: sys.exit('No week column (week_start/week) in picklist.')
          # coerce
          if 'rank' in df.columns: df['rank']=pd.to_numeric(df['rank'], errors='coerce')
          if 'score' in df.columns: df['score']=pd.to_numeric(df['score'], errors='coerce')

          # latest week subset
          w=pd.to_datetime(df[wk], errors='coerce').dt.date
          latest=w.max()
          wdf=df[w==latest].copy()
          if wdf.empty: sys.exit('No rows for latest week.')

          # sort by rules: rank asc if present else score desc; tie-break symbol asc
          if 'rank' in wdf.columns and wdf['rank'].notna().any():
              wdf=wdf.sort_values(['rank','symbol'], ascending=[True,True], kind='stable')
          elif 'score' in wdf.columns and wdf['score'].notna().any():
              wdf=wdf.sort_values(['score','symbol'], ascending=[False,True], kind='stable')

          # sanity prints
          print(f'Latest week: {latest} | rows={len(wdf)}')
          cols=[c for c in ['symbol','rank','score','filters'] if c in wdf.columns]
          print('Top-30 (symbol, rank, score):')
          print(wdf[cols].head(30).to_string(index=False))

          # show first letters (to detect accidental alphabetical picking)
          first_letters=(wdf['symbol'].astype(str).str[0]).value_counts().sort_index()
          print('\nFirst-letter distribution for latest week:')
          print(first_letters.to_string())

          # save a full debug CSV for download
          out='backtests/latest_week_debug.csv'
          wdf.to_csv(out, index=False)
          print(f'\nSaved full latest-week table to {out}')

      - name: Derive WEEK from picklist (compact)
        shell: python
        run: |
          import os, sys, pandas as pd
          path='backtests/picklist_highrsi_trend.csv'; df=pd.read_csv(path)
          if df.empty: print('ERROR: picklist is empty'); sys.exit(1)
          col='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not col: print('ERROR: no week column found'); sys.exit(1)
          dates=pd.to_datetime(df[col], errors='coerce').dropna().dt.date
          if dates.empty: print('ERROR: no parsable dates in', col); sys.exit(1)
          week=max(dates.unique()); print('Chosen WEEK:', week)
          open(os.environ['GITHUB_ENV'],'a').write(f'WEEK={week}\n')

      - name: Compute trailing stops (ratcheting, ATR 14 × 1.75) and write report
        shell: bash
        run: |
          WEEK_VAL="${WEEK:-${{ steps.when.outputs.friday }}}"
          MON_VAL="${{ steps.when.outputs.monday }}"
          echo "Using WEEK=$WEEK_VAL  MONDAY=$MON_VAL"
          python trailing_stops.py \
            --data-dir stock_data_400 \
            --picklist backtests/picklist_highrsi_trend.csv \
            --week "$WEEK_VAL" \
            --monday "$MON_VAL" \
            --topk "${{ env.TOPK }}" \
            --positions backtests/positions.json \
            --method atr \
            --atr-win 14 \
            --atr-mult 1.75

      - name: Upload trailing-stop artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trailing-stops-${{ steps.when.outputs.friday }}
          path: |
            backtests/positions.json
            backtests/stops.csv
            backtests/trailing_report.txt
      
      - name: Upload latest-week debug CSV
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: latest-week-debug-${{ steps.when.outputs.friday }}
          path: backtests/latest_week_debug.csv
          retention-days: 7
      
      - name: Check picklist (exists + head)
        run: |
          ls -lh backtests || true
          python -c "import pandas as pd, pathlib, sys; p=pathlib.Path('backtests/picklist_highrsi_trend.csv'); print('exists:', p.exists(), 'size:', p.stat().st_size if p.exists() else 0); (not p.exists()) and sys.exit(1); df=pd.read_csv(p); print('rows:', len(df)); print(df.head(10).to_string(index=False)); sys.exit(0 if len(df) else 1)"

      - name: Preview Top-6 in logs + save file
        if: ${{ env.MODE == 'build' || env.MODE == 'build_and_email' }}
        run: |
          python preview_top6.py \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk "${{ inputs.topk }}" \
            --until "${{ steps.when.outputs.friday }}" | tee backtests/top6_preview.txt

          echo "== Tail of picklist =="
          tail -n 20 backtests/picklist_highrsi_trend.csv || true

      - name: Save Top-K preview to file
        run: |
          python - <<'PY'
          import pandas as pd, sys
          df = pd.read_csv('backtests/picklist_highrsi_trend.csv')
          wk = "${{ steps.when.outputs.friday }}"
          sub = df[pd.to_datetime(df.get('week_start', df.get('week')), errors='coerce').dt.date.astype(str)==wk]
          cols = [c for c in ['symbol','rank','score','filters'] if c in sub.columns]
          out = sub.sort_values(cols[:1]).head(int("${{ env.TOPK }}"))
          out.to_string(open('backtests/top6_preview.txt','w'), index=False)
          PY

      - name: Upload picklist artifacts
        uses: actions/upload-artifact@v4
        with:
          name: weekly-picks-${{ steps.when.outputs.friday }}
          path: |
            backtests/picklist_highrsi_trend.csv
            backtests/top6_preview.txt
            runner_dates.json

      # --- EMAIL (smoke or real) ---
      - name: Create email config from secrets
        if: ${{ inputs.mode == 'email_smoke' || inputs.mode == 'build_and_email' }}
        env:
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASS: ${{ secrets.GMAIL_APP_PASS }}
          MAIL_FROM: ${{ secrets.MAIL_FROM }}
          MAIL_TO: ${{ secrets.MAIL_TO }}
        run: |
          python - <<'PY'
          import os, json, pathlib, sys, yaml
          out = {
            "smtp": {
              "host": "smtp.gmail.com",
              "port": 587,
              "user": os.environ.get("GMAIL_USER",""),
              "pass": os.environ.get("GMAIL_APP_PASS",""),
            },
            "from": os.environ.get("MAIL_FROM",""),
            # allow comma or semicolon separated
            "to": [e.strip() for e in os.environ.get("MAIL_TO","").replace(";",",").split(",") if e.strip()]
          }
          pathlib.Path("config_notify_ci.yaml").write_text(yaml.safe_dump(out, sort_keys=False), encoding="utf-8")
          print("Wrote config_notify_ci.yaml with", len(out["to"]), "recipient(s).")
          PY

      - name: Email smoke (short test)
        if: ${{ inputs.mode == 'email_smoke' }}
        run: |
          python notify_picks.py --config config_notify_ci.yaml --smoke

      - name: Write config_notify.yaml (from secrets)
        run: |
          cat > config_notify.yaml <<'YAML'
          smtp_user: ${{ secrets.SMTP_USER }}
          smtp_pass: ${{ secrets.SMTP_PASS }}
          smtp_to:   ${{ secrets.SMTP_TO }}
          smtp_from: ${{ secrets.SMTP_FROM }}
          host: smtp.gmail.com
          port: 587
          use_tls: true
          YAML

      - name: Show config_notify.yaml (redacted)
        run: |
          python -c "import yaml, pathlib; cfg=yaml.safe_load(pathlib.Path('config_notify.yaml').read_text()) or {}; print({k:('***' if any(s in k for s in ('pass','user','key','token')) else v) for k,v in cfg.items()})"


      - name: Email weekly picks (poem + full names, BCC recipients)
        if: ${{ env.MODE == 'build_and_email' || env.MODE == 'email_smoke' }}
        env:
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          SMTP_TO:   ${{ secrets.SMTP_TO }}
          SMTP_FROM: ${{ secrets.SMTP_FROM }}
        run: |
          python notify_picks.py \
            --config config_notify.yaml \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk ${{ inputs.topk != '' && inputs.topk || '6' }} \
            --week "${WEEK}"

      - name: Gate to U.S. open (Alpaca clock, max 2h sleep)
        id: gate
        continue-on-error: true
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}        # or your *_API_KEY_ID
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}     # or your *_API_SECRET_KEY
          ALPACA_ENV:    paper
          RUN_OPEN:      ${{ inputs.run_open }}
        shell: python
        run: |
          import os, time, sys, requests, datetime as dt
          key=os.getenv("ALPACA_KEY"); sec=os.getenv("ALPACA_SECRET")
          base="https://paper-api.alpaca.markets" if os.getenv("ALPACA_ENV","paper").startswith("paper") else "https://api.alpaca.markets"
          if os.getenv("RUN_OPEN","false").lower()=="true":
              print("Manual run_open=true — skipping gate and proceeding.")
              open(os.environ["GITHUB_OUTPUT"],"a").write("go=1\n"); sys.exit(0)
          h={"APCA-API-KEY-ID":key,"APCA-API-SECRET-KEY":sec}
          r=requests.get(f"{base}/v2/clock",headers=h,timeout=20); print("Clock status:",r.status_code)
          c=r.json(); print("Clock JSON:", c)
          if c.get("is_open"):
              open(os.environ["GITHUB_OUTPUT"],"a").write("go=1\n"); sys.exit(0)
          next_open=dt.datetime.fromisoformat(c["next_open"].replace("Z","+00:00"))
          now=dt.datetime.now(dt.timezone.utc); wait=(next_open-now).total_seconds()
          print(f"Now(UTC)={now}  next_open={next_open}  wait_sec={int(wait)}")
          if 0 < wait <= 7200:
              time.sleep(int(wait)); open(os.environ["GITHUB_OUTPUT"],"a").write("go=1\n"); sys.exit(0)
          print("Open more than 2h away — neutral go=0")
          open(os.environ["GITHUB_OUTPUT"],"a").write("go=0\n"); sys.exit(0)

      - name: Execute exits at open (if any)
        if: ${{ steps.gate.outputs.go == '1' }}
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}
          ALPACA_ENV:    paper
        shell: bash
        run: |
          if [ -s backtests/exit_symbols.txt ]; then
            echo "Exit list present (backtests/exit_symbols.txt). Triggering sell logic."
            python execute_orders.py \
              --broker alpaca \
              --data-dir stock_data_400 \
              --picklist backtests/picklist_highrsi_trend.csv \
              --week "${WEEK:-${{ steps.when.outputs.friday }}}" \
              --side sell
          else
            echo "No exits today."
          fi
      
      - name: Decide if today is first session after last Friday (and log)
        if: ${{ steps.gate.outputs.go == '1' }}
        id: buygate
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}
          ALPACA_ENV:    paper
        shell: python
        run: |
          import os,requests,datetime as dt
          from zoneinfo import ZoneInfo

          base = "https://paper-api.alpaca.markets" if os.getenv("ALPACA_ENV","paper").startswith("paper") else "https://api.alpaca.markets"
          k = os.getenv("ALPACA_KEY"); s = os.getenv("ALPACA_SECRET")
          assert k and s, f"Missing Alpaca secrets: key={'set' if k else 'MISSING'}, secret={'set' if s else 'MISSING'}"
          h = {"APCA-API-KEY-ID": k, "APCA-API-SECRET-KEY": s}

          friday = "${{ steps.when.outputs.friday }}"
          start  = (dt.date.fromisoformat(friday) + dt.timedelta(days=1)).isoformat()
          end    = (dt.date.fromisoformat(friday) + dt.timedelta(days=7)).isoformat()

          r = requests.get(f"{base}/v2/calendar", params={"start": start, "end": end}, headers=h, timeout=20)
          print("Calendar status:", r.status_code)
          try:
              cal = r.json()
          except Exception:
              print("Calendar body (first 500 chars):\n", r.text[:500])
              raise

          first = (cal[0]["date"] if cal else None)
          today_ny = dt.datetime.now(dt.timezone.utc).astimezone(ZoneInfo("America/New_York")).date().isoformat()
          do = "1" if first and today_ny == first else "0"
          print(f"First session after {friday}: {first} | Today: {today_ny} | BUY={do}")
          open(os.environ["GITHUB_OUTPUT"], "a").write(f"do={do}\n")

      - name: Debug — inspect picklist before buys
        shell: python
        run: |
          import pathlib, pandas as pd, sys
          p = pathlib.Path("backtests/picklist_highrsi_trend.csv")
          if not p.exists():
              print("ERROR: missing", p); sys.exit(1)
          df = pd.read_csv(p)
          print("Picklist rows:", len(df), "columns:", list(df.columns))
          print(df.head(12).to_string(index=False))
          if df.empty:
              print("Picklist is empty — aborting buys.")
              sys.exit(78)

      - name: Ensure picklist exists (build if missing)
        run: |
          mkdir -p backtests
          if [ ! -f backtests/picklist_highrsi_trend.csv ]; then
            echo "Picklist missing — building it now…"
            python build_picklist_parametric.py \
              --data-dir stock_data_400 \
              --rsi-min 68 \
              --max-ext20 0.15 \
              --top-per-week 40 \
              --until "${UNTIL}" \
              --out backtests/picklist_highrsi_trend.csv
             ls -lh backtests/picklist_highrsi_trend.csv
           else
             echo "Picklist already present:"
             ls -lh backtests/picklist_highrsi_trend.csv
           fi

     
      - name: Prepare buy list from picklist (robust week select)
        if: ${{ steps.gate.outputs.go == '1' }}
        shell: python
        run: |
          import os, sys, pathlib, pandas as pd
          path = pathlib.Path("backtests/picklist_highrsi_trend.csv")
          if not path.exists():
              print("ERROR: picklist missing:", path); sys.exit(78)

          df = pd.read_csv(path)
          if df.empty:
             print("Picklist is empty — aborting buys."); sys.exit(78)

          # choose week column
          col = "week_start" if "week_start" in df.columns else ("week" if "week" in df.columns else None)
          if not col:
             print("ERROR: no week column. Columns:", list(df.columns)); sys.exit(1)

          # list available weeks
          ser = pd.to_datetime(df[col], errors="coerce")
          uniq = sorted(set(ser.dropna().dt.date))
          print("Weeks present (last 8):", [d.isoformat() for d in uniq[-8:]])

          # pick WEEK: prefer env WEEK (from 'Derive WEEK'), else latest in file
          week = os.environ.get("WEEK") or (uniq[-1].isoformat() if uniq else None)
          if not week:
             print("ERROR: no parsable weeks in picklist."); sys.exit(1)
          print("Chosen WEEK:", week)

          # filter + sort (rank asc, score desc)
          sel = df[ser.dt.date.astype(str) == week].copy()
          if "rank" in sel.columns:
              sel = sel.sort_values(["rank","score"], ascending=[True, False])
          elif "score" in sel.columns:
              sel = sel.sort_values("score", ascending=False)

          topk = int(os.environ.get("TOPK", "6"))
          picks = sel["symbol"].dropna().astype(str).head(topk).tolist()
          print(f"WEEK={week} | candidates={len(sel)} | picks={picks}")
          if not picks:
              print("No picks for chosen WEEK — aborting buys."); sys.exit(78)

          pathlib.Path("backtests").mkdir(parents=True, exist_ok=True)
          open("backtests/buy_symbols.txt","w").write("\n".join(picks))

      - name: Execute Monday buys (Top-K) with day-1 ATR stop
        if: ${{ steps.gate.outputs.go == '1' && (steps.buygate.outputs.do == '1' || inputs.force_buy == true) }}
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}      # or your *_API_KEY_ID
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}   # or your *_API_SECRET_KEY
          ALPACA_ENV:    paper
          NOTIONAL_PER:  "5000"
        shell: bash
        run: |
          WEEK_VAL="${WEEK:-${{ steps.when.outputs.monday }}}"
          echo "Buying for WEEK=$WEEK_VAL (TOPK=${{ env.TOPK }}) from picklist_highrsi_trend.csv"
          python execute_orders.py \
            --broker alpaca \
            --data-dir stock_data_400 \
            --picklist backtests/picklist_highrsi_trend.csv \
            --week "$WEEK_VAL" \
            --topk "${{ env.TOPK }}" \
            --side buy \
            --notional "${NOTIONAL_PER}" \
            --init-stop-atr-mult 1.75
      
      - name: Upload order log
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: orders-${{ steps.when.outputs.friday }}
          path: backtests/orders_*.json

      - name: Slack — Open recap (only if orders)
        if: ${{ always() }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash
        continue-on-error: true
        run: |
          set -euo pipefail
          if [[ -z "${SLACK_WEBHOOK_URL:-}" ]]; then echo "Slack not configured; skipping."; exit 0; fi
          sudo apt-get update -y >/dev/null && sudo apt-get install -y jq >/dev/null
          files=$(find backtests -maxdepth 1 -type f -name 'orders_*.json' -mmin -360 2>/dev/null || true)
          if [[ -z "$files" ]]; then echo "No recent orders; skipping."; exit 0; fi
          lines=$(jq -r '(if type=="array" then .[] else . end)|[.symbol//.asset_symbol//"?",(.side//"?")|ascii_upcase, (.qty//.filled_qty//"-"),(.filled_avg_price//.limit_price//.stop_price//"-"),(.status//"?")]|@tsv' $files \
           | awk '{printf "%s %s qty=%s px=%s status=%s\n",$2,$1,$3,$4,$5}')
          if [[ -z "$lines" ]]; then echo "Parsed nothing; skipping."; exit 0; fi
          payload=$(jq -Rn --arg t "*Intraweek Bot — Open recap*\n```$lines```" '{text:$t}')
          curl -fsS -X POST -H 'Content-type: application/json' --data "$payload" "$SLACK_WEBHOOK_URL" >/dev/null && echo "Slack message sent."

      - name: Slack — Exits queued (only if any)
        if: ${{ github.event_name == 'schedule' && (github.event.schedule == '15 20 * * 1-4' || github.event.schedule == '15 21 * * 1-4' || github.event.schedule == '15 20 * * 5' || github.event.schedule == '15 21 * * 5') }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash
        continue-on-error: true
        run: |
          set -euo pipefail
          if [[ -z "${SLACK_WEBHOOK_URL:-}" ]]; then echo "Slack not configured; skipping."; exit 0; fi
          if [[ ! -s backtests/exit_symbols.txt ]]; then echo "No exits queued; skipping."; exit 0; fi
          sudo apt-get update -y >/dev/null && sudo apt-get install -y jq >/dev/null
          syms=$(tr '\n' ' ' < backtests/exit_symbols.txt | sed 's/[[:space:]]\+/ /g; s/^ *//; s/ *$//')
          payload=$(jq -Rn --arg t "*Intraweek Bot — Exits queued for next open*\n\`\`\`$syms\`\`\`" '{text:$t}')
          curl -fsS -X POST -H 'Content-type: application/json' --data "$payload" "$SLACK_WEBHOOK_URL" >/dev/null && echo "Slack message sent."
