name: Weekly Picks

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Choose: status, build, email_smoke, build_and_email"
        required: true
        default: build_and_email
        type: choice
        options:
          - status
          - build
          - email_smoke
          - build_and_email
      until:
        description: "Override Friday (YYYY-MM-DD). Leave empty to auto-pick last Friday (Oslo)"
        required: false
        type: string
      topk:
        description: "How many names to show/email"
        required: false
        default: "6"
        type: string

jobs:
  main:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml requests

      - name: Compute last Friday (Europe/Oslo)
        id: when
        run: |
          python - <<'PY'
          from datetime import datetime, timedelta
          import zoneinfo, os, json
          tz = zoneinfo.ZoneInfo("Europe/Oslo")
          now = datetime.now(tz)
          # If user supplied --until, use that, else compute last Friday
          override = "${{ inputs.until }}".strip()
          if override:
            fri = override
          else:
            d = now.date()
            # step back to previous Friday (including today if Fri)
            while d.weekday() != 4:
              d -= timedelta(days=1)
            fri = d.isoformat()
          # next Monday after that Friday
          f = datetime.fromisoformat(fri).date()
          next_mon = f + timedelta(days=3)
          print("FRI=", fri)
          print("MON=", next_mon.isoformat())
          with open("runner_dates.json","w") as fjson:
            json.dump({"friday": fri, "next_monday": next_mon.isoformat()}, fjson)
          print(f"::set-output name=friday::{fri}")
          print(f"::set-output name=monday::{next_mon.isoformat()}")
          PY

      - name: Cache dataset between runs
        uses: actions/cache@v4
        with:
          path: stock_data_400
          key: polygon-dataset-${{ steps.when.outputs.friday }}-max500
          restore-keys: |
            polygon-dataset-

      - name: Fetch equities (Polygon) and build stock_data_400.zip
        env:
          POLY_KEY: ${{ secrets.POLYGON_API_KEY }}
          MAX_SYMBOLS: "500"         # set "400" if you want exactly 400
          SINCE: "2023-01-01"        # how far back to fetch daily bars
          FRIDAY: ${{ steps.when.outputs.friday }}
        shell: python
        run: |
          import os, sys, csv, time, random, glob, zipfile, pathlib, datetime as dt, requests
          from urllib.parse import urlencode

          key = os.environ.get("POLY_KEY")
          if not key:
            print("No POLYGON_API_KEY; skipping Polygon fetch and leaving any existing ZIP as-is.")
            sys.exit(0)

          data_dir = pathlib.Path("stock_data_400"); data_dir.mkdir(exist_ok=True)
          out_zip  = pathlib.Path("stock_data_400.zip")
          friday   = os.environ["FRIDAY"]           # YYYY-MM-DD
          since    = os.environ.get("SINCE", "2023-01-01")
          maxn     = int(os.environ.get("MAX_SYMBOLS", "500"))

          # ---- helper: polite backoff on 429 ----
          def get_with_backoff(url, params, tries=6):
            for k in range(tries):
              r = requests.get(url, params=params, timeout=30)
              if r.status_code != 429:
                r.raise_for_status()
                return r
              retry = r.headers.get("Retry-After")
              if retry is None:
                wait = (0.5 * (2**k)) * random.uniform(0.8, 1.2)
              else:
                wait = float(retry)
              time.sleep(wait)
            r.raise_for_status()

          # ---- 1) build a clean universe: US Common Stock, active on Friday ----
          base = "https://api.polygon.io/v3/reference/tickers"
          params = dict(market="stocks", type="CS", active="true", locale="us", date=friday, limit=1000, apiKey=key)
          syms = []
          url = base
          while True:
            j = get_with_backoff(url, params).json()
            syms += [x["ticker"] for x in j.get("results", [])]
            nxt = j.get("next_url")
            if not nxt: break
            # next_url already includes query; make sure apiKey is present
            url = nxt + ("&" if "?" in nxt else "?") + urlencode({"apiKey": key})
            params = {}

          # default blocklist + repo file (if present)
          defaults = {"SPLK","SGEN","SPY","QQQ","IWM","VOO","VTI","DIA","XLK"}
          exfile = pathlib.Path("universe/exclude.txt")
          exfile.parent.mkdir(parents=True, exist_ok=True)
          extra = set()
          if exfile.exists():
            extra = {s.strip().upper() for s in exfile.read_text(encoding="utf-8").splitlines()
                     if s.strip() and not s.strip().startswith("#")}
          excl = defaults | extra

          syms = [s for s in syms if s not in excl][:maxn]
          print(f"Universe: {len(syms)} symbols (cap={maxn}).")

          # ---- 2) append only missing days per symbol (uses cache if restored) ----
          headers = ["Date","Open","High","Low","Close","Volume"]
          agg_url = "https://api.polygon.io/v2/aggs/ticker/{sym}/range/1/day/{start}/{end}"
          added_rows = 0
          downloaded = 0

          for i, sym in enumerate(syms, 1):
            path = data_dir / f"{sym}.csv"
            # determine start date for incremental update
            start = since
            if path.exists():
              try:
                import pandas as pd
                last = pd.read_csv(path, usecols=[0]).iloc[-1,0]
                last_d = dt.datetime.fromisoformat(str(last)).date()
                start = (last_d + dt.timedelta(days=1)).isoformat()
              except Exception:
                start = since

            if start > friday:
              # already up to date
              continue

            u = agg_url.format(sym=sym, start=start, end=friday)
            rr = get_with_backoff(u, {"adjusted":"true","limit":50000,"sort":"asc","apiKey":key})
            data = rr.json().get("results", [])
            if not data:
              continue

            mode = "a" if path.exists() and start != since else "w"
            with path.open(mode, newline="") as f:
              w = csv.writer(f)
              if mode == "w":
                w.writerow(headers)
              for bar in data:
                d = dt.datetime.utcfromtimestamp(bar["t"]/1000).date().isoformat()
                w.writerow([d, bar["o"], bar["h"], bar["l"], bar["c"], bar["v"]])
                added_rows += 1

            downloaded += 1
            if i % 50 == 0:
              print(f"{i}/{len(syms)} processed …")

            time.sleep(0.05)  # stay polite

          # ---- 3) zip for the next step ----
          with zipfile.ZipFile(out_zip, "w", compression=zipfile.ZIP_DEFLATED) as z:
            for p in data_dir.glob("*.csv"):
              z.write(p, arcname=p.name)

          print(f"Built {out_zip}  files={len(list(data_dir.glob('*.csv')))}  updated_syms={downloaded}  new_rows={added_rows}")

      # Always show a quick repo tree to help debugging
      - name: Quick tree
        run: |
          echo "Top-level files:"
          ls -la
          echo ""
          echo "Data dir (stock_data_400):"
          ls -la stock_data_400 | head -n 40 || true

      # --- STATUS (sanity) ---
      - name: Status (sanity only)
        if: ${{ inputs.mode == 'status' }}
        run: |
          echo "Sanity OK. CSV count:"
          ls stock_data_400/*.csv | wc -l

      - name: Unpack EOD dataset if zip present
        if: hashFiles('stock_data_400.zip') != ''
        shell: python
        run: |
          import os, zipfile, shutil, pathlib

          z = zipfile.ZipFile('stock_data_400.zip')
          os.makedirs('stock_data_400', exist_ok=True)
          z.extractall('stock_data_400')

          moved = 0
          for p in pathlib.Path('stock_data_400').rglob('*.csv'):
             dst = pathlib.Path('stock_data_400') / p.name
             if p.resolve() != dst.resolve():
                shutil.move(str(p), str(dst))
                moved += 1

          csvs = [p for p in os.listdir('stock_data_400') if p.lower().endswith('.csv')]
          print("Unzipped CSVs:", len(csvs), "(moved:", moved, ")")

      - name: Generate equities-only universe (from dataset + exclude list)
        shell: python
        run: |
          import os, csv, glob, pathlib, datetime as dt
          import pandas as pd

          data_dir = "stock_data_400"
          uni_dir  = pathlib.Path("universe"); uni_dir.mkdir(exist_ok=True, parents=True)
          exclude_file = uni_dir / "exclude.txt"
          out_file = uni_dir / "top400_weekly.generated.csv"

          # Make sure exclude list exists (you can edit it in the repo anytime)
          if not exclude_file.exists():
              exclude_file.write_text(
                  "# One symbol per line (UPPERCASE). ETFs/funds/delisted here are dropped every run.\n"
                  "SPY\nQQQ\nIWM\nVOO\nVTI\nDIA\nXLK\nXLE\nXLV\nXLF\n"
                  "SPLK\nSGEN\n", encoding="utf-8"
              )

          # 1) collect symbols from filenames in dataset (handles TICKER.csv or TICKER_*.csv)
          symbols = set()
          for p in glob.glob(os.path.join(data_dir, "*.csv")):
              base = pathlib.Path(p).stem
              sym = base.split("_")[0].upper()
              symbols.add(sym)

          # 2) remove excluded
          exclude = {line.strip().upper() for line in exclude_file.read_text(encoding="utf-8").splitlines()
                     if line.strip() and not line.strip().startswith("#")}
          symbols -= exclude

          # 3) drop stale/delisted: if last row date is older than cutoff (e.g., 120 days before dataset max)
          last_dates = {}
          dataset_last = dt.date(1970,1,1)
          for sym in list(symbols):
              # try both TICKER.csv and TICKER_*.csv
              candidates = sorted(glob.glob(os.path.join(data_dir, f"{sym}*.csv")))
              if not candidates: continue
              try:
                  df = pd.read_csv(candidates[0])
                  if df.empty: continue
                  # assume first column is date
                  d = pd.to_datetime(df.iloc[-1, 0], errors="coerce")
                  if pd.notna(d):
                      dd = d.date()
                      last_dates[sym] = dd
                      if dd > dataset_last: dataset_last = dd
              except Exception:
                  pass

          cutoff = dataset_last - dt.timedelta(days=120)
          symbols = {s for s in symbols if s in last_dates and last_dates[s] >= cutoff}

          # 4) write universe CSV
          with out_file.open("w", newline="") as f:
              w = csv.DictWriter(f, fieldnames=["symbol"])
              w.writeheader()
              for s in sorted(symbols):
                  w.writerow({"symbol": s})

          print(f"Universe → {out_file}  kept={len(symbols)}  excluded={len(exclude)}  cutoff={cutoff}")
          if len(symbols) == 0:
              raise SystemExit("Universe is empty after filtering; aborting.")

      - name: Sanity — count & peek one CSV
        shell: python
        run: |
          import glob, pandas as pd
          files = sorted(glob.glob('stock_data_400/*.csv'))
          print("Found", len(files), "CSV files in stock_data_400/")
          if not files:
             raise SystemExit("No CSVs found after unzip")

          print("Sample:", files[0])
          df = pd.read_csv(files[0]).head()
          print(df.to_string(index=False))

      # --- BUILD PICKLIST ---
      - name: Build weekly picklist (RSI≥68, rank=rsi, ext20≤0.15, Top-40/wk)
        run: |
          python build_picklist_parametric.py \
            --data-dir stock_data_400 \
            --rsi-min 68 \
            --max-ext20 0.15 \
            --top-per-week 40 \
            --until "${UNTIL}" \
            --out backtests/picklist_highrsi_trend.csv

      - name: Derive WEEK from picklist (compact)
        shell: python
        run: |
          import os, sys, pandas as pd
          path='backtests/picklist_highrsi_trend.csv'; df=pd.read_csv(path)
          if df.empty: print('ERROR: picklist is empty'); sys.exit(1)
          col='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not col: print('ERROR: no week column found'); sys.exit(1)
          dates=pd.to_datetime(df[col], errors='coerce').dropna().dt.date
          if dates.empty: print('ERROR: no parsable dates in', col); sys.exit(1)
          week=max(dates.unique()); print('Chosen WEEK:', week)
          open(os.environ['GITHUB_ENV'],'a').write(f'WEEK={week}\n')
      
      - name: Check picklist (exists + head)
        run: |
          ls -lh backtests || true
          python -c "import pandas as pd, pathlib, sys; p=pathlib.Path('backtests/picklist_highrsi_trend.csv'); print('exists:', p.exists(), 'size:', p.stat().st_size if p.exists() else 0); (not p.exists()) and sys.exit(1); df=pd.read_csv(p); print('rows:', len(df)); print(df.head(10).to_string(index=False)); sys.exit(0 if len(df) else 1)"

      - name: Preview Top-6 in logs + save file
        if: ${{ inputs.mode == 'build' || inputs.mode == 'build_and_email' }}
        run: |
          python preview_top6.py \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk "${{ inputs.topk }}" \
            --until "${{ steps.when.outputs.friday }}" | tee backtests/top6_preview.txt

          echo "== Tail of picklist =="
          tail -n 20 backtests/picklist_highrsi_trend.csv || true

      - name: Upload artifacts (picklist + preview)
        if: ${{ inputs.mode == 'build' || inputs.mode == 'build_and_email' }}
        uses: actions/upload-artifact@v4
        with:
          name: weekly-picks-${{ steps.when.outputs.friday }}
          path: |
            backtests/picklist_highrsi_trend.csv
            backtests/top6_preview.txt
            runner_dates.json

      # --- EMAIL (smoke or real) ---
      - name: Create email config from secrets
        if: ${{ inputs.mode == 'email_smoke' || inputs.mode == 'build_and_email' }}
        env:
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASS: ${{ secrets.GMAIL_APP_PASS }}
          MAIL_FROM: ${{ secrets.MAIL_FROM }}
          MAIL_TO: ${{ secrets.MAIL_TO }}
        run: |
          python - <<'PY'
          import os, json, pathlib, sys, yaml
          out = {
            "smtp": {
              "host": "smtp.gmail.com",
              "port": 587,
              "user": os.environ.get("GMAIL_USER",""),
              "pass": os.environ.get("GMAIL_APP_PASS",""),
            },
            "from": os.environ.get("MAIL_FROM",""),
            # allow comma or semicolon separated
            "to": [e.strip() for e in os.environ.get("MAIL_TO","").replace(";",",").split(",") if e.strip()]
          }
          pathlib.Path("config_notify_ci.yaml").write_text(yaml.safe_dump(out, sort_keys=False), encoding="utf-8")
          print("Wrote config_notify_ci.yaml with", len(out["to"]), "recipient(s).")
          PY

      - name: Email smoke (short test)
        if: ${{ inputs.mode == 'email_smoke' }}
        run: |
          python notify_picks.py --config config_notify_ci.yaml --smoke

      - name: Write config_notify.yaml (from secrets)
        run: |
          cat > config_notify.yaml <<'YAML'
          smtp_user: ${{ secrets.SMTP_USER }}
          smtp_pass: ${{ secrets.SMTP_PASS }}
          smtp_to:   ${{ secrets.SMTP_TO }}
          smtp_from: ${{ secrets.SMTP_FROM }}
          host: smtp.gmail.com
          port: 587
          use_tls: true
          YAML

      - name: Show config_notify.yaml (redacted)
        run: |
          python -c "import yaml, pathlib; cfg=yaml.safe_load(pathlib.Path('config_notify.yaml').read_text()) or {}; print({k:('***' if any(s in k for s in ('pass','user','key','token')) else v) for k,v in cfg.items()})"


      - name: Email weekly picks (poem + full names, BCC recipients)
        env:
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          SMTP_TO:   ${{ secrets.SMTP_TO }}
          SMTP_FROM: ${{ secrets.SMTP_FROM }}
        run: |
          python notify_picks.py \
            --config config_notify.yaml \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk ${{ inputs.topk != '' && inputs.topk || '6' }} \
            --week "${WEEK}"
