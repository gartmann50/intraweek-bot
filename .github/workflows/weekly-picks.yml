name: Weekly Picks

on:
  # Auto: every Friday 20:15 UTC  ≈ 22:15 in Oslo during summer
  # (in winter this fires at 21:15 Oslo; see note below)
  schedule:
    - cron: "15 20 * * 5"
    - cron: "15 21 * * 5"
    - cron: "28 13 * * 1"      # when U.S. is on EDT (open = 13:30 UTC)
    - cron: "28 14 * * 1"      # when U.S. is on EST (open = 14:30 UTC)

  # Manual: keep your existing inputs
  workflow_dispatch:
    inputs:
      mode:
        description: "Choose: status, build, email_smoke, build_and_email"
        required: true
        default: build_and_email
        type: choice
        options:
          - status
          - build
          - email_smoke
          - build_and_email
      until:
        description: "Override Friday (YYYY-MM-DD). Leave empty to auto-pick last Friday (Oslo)"
        required: false
        type: string
      topk:
        description: "How many names to show/email"
        required: false
        default: "6"
        type: string
      
concurrency:
  group: weekly-picks
  cancel-in-progress: true

jobs:
  main:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    # Defaults for scheduled runs
    env:
      MODE:  ${{ github.event_name == 'schedule' && 'build_and_email' || inputs.mode }}
      TOPK:  ${{ inputs.topk != '' && inputs.topk || '6' }}
      UNTIL: ${{ inputs.until }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml requests

      - name: Echo trigger
        shell: bash
        run: |
          echo "Triggered by: ${{ github.event_name }} at $(/bin/date -u '+%Y-%m-%d %H:%M:%S') UTC"

      - name: Compute last Friday (Europe/Oslo)
        id: when
        run: |
          python - <<'PY'
          from datetime import datetime, timedelta
          import zoneinfo, os, json
          tz = zoneinfo.ZoneInfo("Europe/Oslo")
          now = datetime.now(tz)
          # If user supplied --until, use that, else compute last Friday
          override = "${{ inputs.until }}".strip()
          if override:
            fri = override
          else:
            d = now.date()
            # step back to previous Friday (including today if Fri)
            while d.weekday() != 4:
              d -= timedelta(days=1)
            fri = d.isoformat()
          # next Monday after that Friday
          f = datetime.fromisoformat(fri).date()
          next_mon = f + timedelta(days=3)
          print("FRI=", fri)
          print("MON=", next_mon.isoformat())
          with open("runner_dates.json","w") as fjson:
            json.dump({"friday": fri, "next_monday": next_mon.isoformat()}, fjson)
          print(f"::set-output name=friday::{fri}")
          print(f"::set-output name=monday::{next_mon.isoformat()}")
          PY

      - name: Gate to U.S. open (Alpaca clock, compact)
        if: ${{ github.event_name == 'schedule' }}
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}
          ALPACA_ENV:    paper
        shell: python
        run: |
            import os,time,requests,datetime as dt
            base = "https://paper-api.alpaca.markets" if os.getenv("ALPACA_ENV","paper").startswith("paper") else "https://api.alpaca.markets"
            h = {"APCA-API-KEY-ID": os.environ["ALPACA_KEY"], "APCA-API-SECRET-KEY": os.environ["ALPACA_SECRET"]}
            c = requests.get(f"{base}/v2/clock", headers=h, timeout=20).json()
            print("is_open:", c.get("is_open"), "next_open:", c.get("next_open"))
            if not c.get("is_open"):
              t = dt.datetime.fromisoformat(c["next_open"].replace("Z","+00:00"))
              s = max(0, (t - dt.datetime.now(dt.timezone.utc)).total_seconds() + 3)
             print(f"sleep {s:.1f}s → bell"); time.sleep(s)

      - name: Execute open buys (Alpaca)
        if: ${{ github.event_name == 'schedule' }}
        env:
          ALPACA_KEY:    ${{ secrets.ALPACA_KEY }}
          ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}
          ALPACA_ENV:    paper
          NOTIONAL_PER:  "5000"
        shell: bash
        run: |
          WEEK_VAL="${WEEK:-${{ steps.when.outputs.friday }}}"
          python execute_orders.py \
            --broker alpaca \
            --data-dir stock_data_400 \
            --picklist backtests/picklist_highrsi_trend.csv \
            --week "$WEEK_VAL" \
            --topk "${{ env.TOPK }}" \
            --side buy \
            --notional "${NOTIONAL_PER}" \
            --init-stop-atr-mult 1.75

      - name: Upload order log
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: orders-${{ steps.when.outputs.friday }}
          path: backtests/orders_*.json

      - name: Cache dataset between runs
        uses: actions/cache@v4
        with:
          path: stock_data_400
          key: polygon-dataset-${{ steps.when.outputs.friday }}-max500
          restore-keys: |
            polygon-dataset-

      - name: Fetch dataset for curated universe (build stock_data_400.zip)
        env:
          POLY_KEY: ${{ secrets.POLYGON_API_KEY }}
          FRIDAY:   ${{ steps.when.outputs.friday }}
          SINCE:    "2023-01-01"
        shell: python
        run: |
          import os, csv, time, zipfile, pathlib, datetime as dt, requests, pandas as pd
          key=os.getenv("POLY_KEY"); friday=os.getenv("FRIDAY"); since=os.getenv("SINCE","2023-01-01")
          if not key: raise SystemExit("Missing POLYGON_API_KEY")

          # 1) read curated allow-list (must exist)
          u=pathlib.Path("universe/top400_weekly.csv")
          if not u.exists(): raise SystemExit("universe/top400_weekly.csv not found (commit it to the repo)")
          syms=pd.read_csv(u)["symbol"].astype(str).str.upper().dropna().unique().tolist()
          print(f"Curated universe: {len(syms)} symbols from {u}")

          # 2) download/append daily bars for each symbol
          ddir=pathlib.Path("stock_data_400"); ddir.mkdir(exist_ok=True)
          headers=["Date","Open","High","Low","Close","Volume"]
          agg="https://api.polygon.io/v2/aggs/ticker/{sym}/range/1/day/{start}/{end}"

          def fetch(sym, start, end):
            for k in range(6):
              r=requests.get(agg.format(sym=sym,start=start,end=end),
                             params={"adjusted":"true","limit":50000,"sort":"asc","apiKey":key}, timeout=30)
              if r.status_code!=429:
                if r.status_code==404: return []  # skip unknown/delisted
                r.raise_for_status(); return r.json().get("results",[]) or []
              time.sleep(0.5*(2**k))
            return []

          added=0; updated=0
          for i,s in enumerate(syms,1):
            p=ddir/f"{s}.csv"
            start=since
            if p.exists():
              try:
                last=pd.read_csv(p,usecols=[0]).iloc[-1,0]
                start=(pd.to_datetime(last).date()+dt.timedelta(days=1)).isoformat()
              except Exception: start=since
            if start>friday: continue
            rows=fetch(s, start, friday)
            if not rows: continue
            mode="a" if p.exists() and start!=since else "w"
            with p.open(mode, newline="") as f:
              w=csv.writer(f)
              if mode=="w": w.writerow(headers)
              for bar in rows:
                d=dt.datetime.utcfromtimestamp(bar["t"]/1000).date().isoformat()
                w.writerow([d,bar["o"],bar["h"],bar["l"],bar["c"],bar["v"]])
                added+=1
            updated+=1
            if i%50==0: print(f"{i}/{len(syms)} processed …")
            time.sleep(0.05)

          # 3) zip for the next step
          z=pathlib.Path("stock_data_400.zip")
          with zipfile.ZipFile(z,"w",compression=zipfile.ZIP_DEFLATED) as zipf:
            for p in ddir.glob("*.csv"): zipf.write(p, arcname=p.name)
          print(f"Built {z} | files={len(list(ddir.glob('*.csv')))} | updated_syms={updated} | new_rows={added}")

      - name: Unpack EOD dataset if zip present
        if: hashFiles('stock_data_400.zip') != ''
        shell: python
        run: |
          import os, zipfile, shutil, pathlib

          z = zipfile.ZipFile('stock_data_400.zip')
          os.makedirs('stock_data_400', exist_ok=True)
          z.extractall('stock_data_400')

          moved = 0
          for p in pathlib.Path('stock_data_400').rglob('*.csv'):
             dst = pathlib.Path('stock_data_400') / p.name
             if p.resolve() != dst.resolve():
                shutil.move(str(p), str(dst))
                moved += 1

          csvs = [p for p in os.listdir('stock_data_400') if p.lower().endswith('.csv')]
          print("Unzipped CSVs:", len(csvs), "(moved:", moved, ")")

      - name: Generate equities-only universe (from dataset + exclude list)
        shell: python
        run: |
          import os, csv, glob, pathlib, datetime as dt
          import pandas as pd

          data_dir = "stock_data_400"
          uni_dir  = pathlib.Path("universe"); uni_dir.mkdir(exist_ok=True, parents=True)
          exclude_file = uni_dir / "exclude.txt"
          out_file = uni_dir / "top400_weekly.generated.csv"

          # Make sure exclude list exists (you can edit it in the repo anytime)
          if not exclude_file.exists():
              exclude_file.write_text(
                  "# One symbol per line (UPPERCASE). ETFs/funds/delisted here are dropped every run.\n"
                  "SPY\nQQQ\nIWM\nVOO\nVTI\nDIA\nXLK\nXLE\nXLV\nXLF\n"
                  "SPLK\nSGEN\n", encoding="utf-8"
              )

          # 1) collect symbols from filenames in dataset (handles TICKER.csv or TICKER_*.csv)
          symbols = set()
          for p in glob.glob(os.path.join(data_dir, "*.csv")):
              base = pathlib.Path(p).stem
              sym = base.split("_")[0].upper()
              symbols.add(sym)

          # 2) remove excluded
          exclude = {line.strip().upper() for line in exclude_file.read_text(encoding="utf-8").splitlines()
                     if line.strip() and not line.strip().startswith("#")}
          symbols -= exclude

          # 3) drop stale/delisted: if last row date is older than cutoff (e.g., 120 days before dataset max)
          last_dates = {}
          dataset_last = dt.date(1970,1,1)
          for sym in list(symbols):
              # try both TICKER.csv and TICKER_*.csv
              candidates = sorted(glob.glob(os.path.join(data_dir, f"{sym}*.csv")))
              if not candidates: continue
              try:
                  df = pd.read_csv(candidates[0])
                  if df.empty: continue
                  # assume first column is date
                  d = pd.to_datetime(df.iloc[-1, 0], errors="coerce")
                  if pd.notna(d):
                      dd = d.date()
                      last_dates[sym] = dd
                      if dd > dataset_last: dataset_last = dd
              except Exception:
                  pass

          cutoff = dataset_last - dt.timedelta(days=120)
          symbols = {s for s in symbols if s in last_dates and last_dates[s] >= cutoff}

          # 4) write universe CSV
          with out_file.open("w", newline="") as f:
              w = csv.DictWriter(f, fieldnames=["symbol"])
              w.writeheader()
              for s in sorted(symbols):
                  w.writerow({"symbol": s})

          print(f"Universe → {out_file}  kept={len(symbols)}  excluded={len(exclude)}  cutoff={cutoff}")
          if len(symbols) == 0:
              raise SystemExit("Universe is empty after filtering; aborting.")

      - name: Sanity — count & peek one CSV
        shell: python
        run: |
          import glob, pandas as pd
          files = sorted(glob.glob('stock_data_400/*.csv'))
          print("Found", len(files), "CSV files in stock_data_400/")
          if not files:
             raise SystemExit("No CSVs found after unzip")

          print("Sample:", files[0])
          df = pd.read_csv(files[0]).head()
          print(df.to_string(index=False))

      # --- BUILD PICKLIST ---
      - name: Build weekly picklist (RSI≥68, rank=rsi, ext20≤0.15, Top-40/wk)
        run: |
          python build_picklist_parametric.py \
            --data-dir stock_data_400 \
            --rsi-min 68 \
            --max-ext20 0.15 \
            --top-per-week 40 \
            --until "${UNTIL}" \
            --out backtests/picklist_highrsi_trend.csv

      - name: Filter to US Common Stock (type=CS) + cap floor, then re-rank (compact)
        env:
          POLY_KEY: ${{ secrets.POLYGON_API_KEY }}
          FRIDAY:   ${{ steps.when.outputs.friday }}
          CAP_MIN:  "10000000000"   # $10B; bump to 20000000000 for $20B if you like
          EXCLUDE_SYMS: "ATLCL,SPLK,SGEN,SPY,QQQ,IWM,VOO,VTI,DIA,XLK"
        shell: python
        run: |
          import os, time, requests, pandas as pd, sys
          pick='backtests/picklist_highrsi_trend.csv'
          try: df=pd.read_csv(pick)
          except Exception as e: sys.exit(f'Could not read {pick}: {e}')
          if df.empty: sys.exit('Picklist is empty.')
          wk='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not wk: sys.exit('No week column.')

          key=os.getenv('POLY_KEY'); friday=os.getenv('FRIDAY'); cap_min=float(os.getenv('CAP_MIN','0'))
          if not key: sys.exit('No POLYGON_API_KEY secret')

          # ban list (defaults + repo file + env)
          ban={s.strip().upper() for s in os.getenv('EXCLUDE_SYMS','').split(',') if s.strip()}
          try:
            with open('universe/exclude.txt',encoding='utf-8') as f:
              ban |= {l.strip().upper() for l in f if l.strip() and not l.strip().startswith('#')}
          except FileNotFoundError:
            pass

          # latest week subset
          df['symbol']=df['symbol'].astype(str).str.upper()
          d=pd.to_datetime(df[wk], errors='coerce').dt.date
          latest=d.max()
          cand=df[d==latest].copy()
          uniq=list(cand['symbol'].dropna().unique())
          # query Polygon overview only for candidates (fast)
          def overview(sym):
            u=f'https://api.polygon.io/v3/reference/tickers/{sym}'
            for k in range(5):
              r=requests.get(u, params={'date':friday,'apiKey':key}, timeout=20)
              if r.status_code==404: return ('NA', 0.0)      # treat as not allowed
              if r.status_code!=429:
                r.raise_for_status()
                j=(r.json() or {}).get('results',{}) or {}
                return (j.get('type'), float(j.get('market_cap') or 0.0))
              time.sleep(0.5*(2**k))
            return ('ERR', 0.0)

          keep=set(); dropped=[]
          for s in uniq:
            if s in ban: dropped.append((s,'BANNED',None)); continue
            t, mc = overview(s)
            if t=='CS' and mc>=cap_min: keep.add(s)
            else: dropped.append((s,t,mc))

          before=len(cand)
          cand=cand[cand['symbol'].isin(keep)].copy()
          removed=before-len(cand)

          # enforce your RSI ordering and renumber ranks for the latest week
          if 'rank' in cand.columns:
            cand['rank']=pd.to_numeric(cand['rank'], errors='coerce')
            cand=cand.sort_values(['rank','symbol'], ascending=[True,True], kind='stable')
          elif 'score' in cand.columns:
            cand['score']=pd.to_numeric(cand['score'], errors='coerce')
            cand=cand.sort_values(['score','symbol'], ascending=[False,True], kind='stable')
          cand['rank']=range(1, len(cand)+1)

          # stitch back into the full file
          out=pd.concat([df[d!=latest], cand], ignore_index=True)
          out.to_csv(pick, index=False)

          print(f'Kept {len(keep)} symbols with type=CS & market_cap >= {int(cap_min):,}. Removed {removed}.')
          if dropped:
            print('Dropped (sample):', dropped[:10])
          print('Top-6 after filter:', ','.join(cand.head(6)['symbol'].astype(str)))

      - name: Debug — show latest-week candidates (sorted) + save CSV
        shell: python
        run: |
          import sys, pandas as pd, numpy as np
          p='backtests/picklist_highrsi_trend.csv'
          try:
              df=pd.read_csv(p)
          except Exception as e:
              sys.exit(f'Could not read {p}: {e}')
          if df.empty: sys.exit('Picklist is empty.')

          # detect week column
          wk='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not wk: sys.exit('No week column (week_start/week) in picklist.')
          # coerce
          if 'rank' in df.columns: df['rank']=pd.to_numeric(df['rank'], errors='coerce')
          if 'score' in df.columns: df['score']=pd.to_numeric(df['score'], errors='coerce')

          # latest week subset
          w=pd.to_datetime(df[wk], errors='coerce').dt.date
          latest=w.max()
          wdf=df[w==latest].copy()
          if wdf.empty: sys.exit('No rows for latest week.')

          # sort by rules: rank asc if present else score desc; tie-break symbol asc
          if 'rank' in wdf.columns and wdf['rank'].notna().any():
              wdf=wdf.sort_values(['rank','symbol'], ascending=[True,True], kind='stable')
          elif 'score' in wdf.columns and wdf['score'].notna().any():
              wdf=wdf.sort_values(['score','symbol'], ascending=[False,True], kind='stable')

          # sanity prints
          print(f'Latest week: {latest} | rows={len(wdf)}')
          cols=[c for c in ['symbol','rank','score','filters'] if c in wdf.columns]
          print('Top-30 (symbol, rank, score):')
          print(wdf[cols].head(30).to_string(index=False))

          # show first letters (to detect accidental alphabetical picking)
          first_letters=(wdf['symbol'].astype(str).str[0]).value_counts().sort_index()
          print('\nFirst-letter distribution for latest week:')
          print(first_letters.to_string())

          # save a full debug CSV for download
          out='backtests/latest_week_debug.csv'
          wdf.to_csv(out, index=False)
          print(f'\nSaved full latest-week table to {out}')

      - name: Compute trailing stops (ratcheting) and write report
        shell: bash
        run: |
          WEEK_VAL="${WEEK:-${{ steps.when.outputs.friday }}}"
          MON_VAL="${{ steps.when.outputs.monday }}"
          echo "Using WEEK=$WEEK_VAL  MONDAY=$MON_VAL"
          python trailing_stops.py \
            --data-dir stock_data_400 \
            --picklist backtests/picklist_highrsi_trend.csv \
            --week "$WEEK_VAL" \
            --monday "$MON_VAL" \
            --topk "${{ env.TOPK }}" \
            --positions backtests/positions.json \
            --method pct \
            --trail-pct 0.10

      - name: Upload trailing-stop artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trailing-stops-${{ steps.when.outputs.friday }}
          path: |
            backtests/positions.json
            backtests/stops.csv
            backtests/trailing_report.txt

      - name: Upload latest-week debug CSV
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: latest-week-debug-${{ steps.when.outputs.friday }}
          path: backtests/latest_week_debug.csv
          retention-days: 7
      
      - name: Derive WEEK from picklist (compact)
        shell: python
        run: |
          import os, sys, pandas as pd
          path='backtests/picklist_highrsi_trend.csv'; df=pd.read_csv(path)
          if df.empty: print('ERROR: picklist is empty'); sys.exit(1)
          col='week_start' if 'week_start' in df.columns else ('week' if 'week' in df.columns else None)
          if not col: print('ERROR: no week column found'); sys.exit(1)
          dates=pd.to_datetime(df[col], errors='coerce').dropna().dt.date
          if dates.empty: print('ERROR: no parsable dates in', col); sys.exit(1)
          week=max(dates.unique()); print('Chosen WEEK:', week)
          open(os.environ['GITHUB_ENV'],'a').write(f'WEEK={week}\n')
      
      - name: Check picklist (exists + head)
        run: |
          ls -lh backtests || true
          python -c "import pandas as pd, pathlib, sys; p=pathlib.Path('backtests/picklist_highrsi_trend.csv'); print('exists:', p.exists(), 'size:', p.stat().st_size if p.exists() else 0); (not p.exists()) and sys.exit(1); df=pd.read_csv(p); print('rows:', len(df)); print(df.head(10).to_string(index=False)); sys.exit(0 if len(df) else 1)"

      - name: Preview Top-6 in logs + save file
        if: ${{ env.MODE == 'build' || env.MODE == 'build_and_email' }}
        run: |
          python preview_top6.py \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk "${{ inputs.topk }}" \
            --until "${{ steps.when.outputs.friday }}" | tee backtests/top6_preview.txt

          echo "== Tail of picklist =="
          tail -n 20 backtests/picklist_highrsi_trend.csv || true

      - name: Upload artifacts (picklist + preview)
        if: ${{ inputs.mode == 'build' || inputs.mode == 'build_and_email' }}
        uses: actions/upload-artifact@v4
        with:
          name: weekly-picks-${{ steps.when.outputs.friday }}
          path: |
            backtests/picklist_highrsi_trend.csv
            backtests/top6_preview.txt
            runner_dates.json

      # --- EMAIL (smoke or real) ---
      - name: Create email config from secrets
        if: ${{ inputs.mode == 'email_smoke' || inputs.mode == 'build_and_email' }}
        env:
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASS: ${{ secrets.GMAIL_APP_PASS }}
          MAIL_FROM: ${{ secrets.MAIL_FROM }}
          MAIL_TO: ${{ secrets.MAIL_TO }}
        run: |
          python - <<'PY'
          import os, json, pathlib, sys, yaml
          out = {
            "smtp": {
              "host": "smtp.gmail.com",
              "port": 587,
              "user": os.environ.get("GMAIL_USER",""),
              "pass": os.environ.get("GMAIL_APP_PASS",""),
            },
            "from": os.environ.get("MAIL_FROM",""),
            # allow comma or semicolon separated
            "to": [e.strip() for e in os.environ.get("MAIL_TO","").replace(";",",").split(",") if e.strip()]
          }
          pathlib.Path("config_notify_ci.yaml").write_text(yaml.safe_dump(out, sort_keys=False), encoding="utf-8")
          print("Wrote config_notify_ci.yaml with", len(out["to"]), "recipient(s).")
          PY

      - name: Email smoke (short test)
        if: ${{ inputs.mode == 'email_smoke' }}
        run: |
          python notify_picks.py --config config_notify_ci.yaml --smoke

      - name: Write config_notify.yaml (from secrets)
        run: |
          cat > config_notify.yaml <<'YAML'
          smtp_user: ${{ secrets.SMTP_USER }}
          smtp_pass: ${{ secrets.SMTP_PASS }}
          smtp_to:   ${{ secrets.SMTP_TO }}
          smtp_from: ${{ secrets.SMTP_FROM }}
          host: smtp.gmail.com
          port: 587
          use_tls: true
          YAML

      - name: Show config_notify.yaml (redacted)
        run: |
          python -c "import yaml, pathlib; cfg=yaml.safe_load(pathlib.Path('config_notify.yaml').read_text()) or {}; print({k:('***' if any(s in k for s in ('pass','user','key','token')) else v) for k,v in cfg.items()})"


      - name: Email weekly picks (poem + full names, BCC recipients)
        if: ${{ env.MODE == 'build_and_email' || env.MODE == 'email_smoke' }}
        env:
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          SMTP_TO:   ${{ secrets.SMTP_TO }}
          SMTP_FROM: ${{ secrets.SMTP_FROM }}
        run: |
          python notify_picks.py \
            --config config_notify.yaml \
            --picklist backtests/picklist_highrsi_trend.csv \
            --topk ${{ inputs.topk != '' && inputs.topk || '6' }} \
            --week "${WEEK}"
